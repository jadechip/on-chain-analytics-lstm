{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metrics",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HgLDnmnGbg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import json\n",
        "import requests\n",
        "import functools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFxN7dkbJsrG",
        "colab_type": "text"
      },
      "source": [
        "## Bitcoin price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foTVepq3Vm0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_request(url, *args):\n",
        "  print(url(*args))\n",
        "  return requests.get(url(*args))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l11fDubusSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start      = 20100718\n",
        "end        = 20180429\n",
        "params     = \"PriceUSD\"\n",
        "url        = lambda params: f\"https://community-api.coinmetrics.io/v2/assets/btc/metricdata?metrics={params}&start={start}&end={end}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8fm4VPiuE-7",
        "colab_type": "code",
        "outputId": "2870dd14-6b92-45c3-9e4d-b795103bd9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "response = make_request(url, \"PriceUSD\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://community-api.coinmetrics.io/v2/assets/btc/metricdata?metrics=PriceUSD&start=20100718&end=20180429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWRIuudJFsIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json = response.json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWT7fFpn0yVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price_df = pd.DataFrame(json[\"metricData\"][\"series\"])\n",
        "price_df[\"values\"] = price_df[\"values\"].apply(lambda x: x[0])\n",
        "price_df[\"values\"] = price_df[\"values\"].astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te6Gl31rr9eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price_df['time'] = pd.to_datetime(price_df['time'])\n",
        "price_df['time'] = price_df['time'].dt.strftime('%d-%m-%Y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzquzcjhYZeB",
        "colab_type": "code",
        "outputId": "39bfd08a-4d62-4d91-b169-4aa899fa5e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "price_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18-07-2010</td>\n",
              "      <td>0.085840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19-07-2010</td>\n",
              "      <td>0.080800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20-07-2010</td>\n",
              "      <td>0.074736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21-07-2010</td>\n",
              "      <td>0.079193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22-07-2010</td>\n",
              "      <td>0.058470</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         time    values\n",
              "0  18-07-2010  0.085840\n",
              "1  19-07-2010  0.080800\n",
              "2  20-07-2010  0.074736\n",
              "3  21-07-2010  0.079193\n",
              "4  22-07-2010  0.058470"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9FR38EHJxj-",
        "colab_type": "text"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4TXYlYsJ6PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start      = 1279324800\n",
        "end        = 1524960000\n",
        "metrics_df = pd.DataFrame()\n",
        "api_key    = \"5a0cf8d7-d14a-44f4-b063-a76807cd5340\"\n",
        "base       = \"https://api.glassnode.com/v1\"\n",
        "url        = lambda api_key: f\"{endpoint}?api_key={api_key}&a=BTC&s={start}&u={end}\"\n",
        "endpoints  = {\n",
        "  \"dormancy\" : f\"{base}/metrics/indicators/average_dormancy\",\n",
        "  \"velocity\" : f\"{base}/metrics/indicators/velocity\",\n",
        "  \"nvts\"     : f\"{base}/metrics/indicators/nvts\",\n",
        "  \"sopr\"     : f\"{base}/metrics/indicators/sopr\",\n",
        "  \"mvrv\"     : f\"{base}/metrics/market/mvrv\" \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGmDCxDMKTRd",
        "colab_type": "code",
        "outputId": "85fb0608-f540-45ff-d664-3f7871d7c017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for name, endpoint in endpoints.items():\n",
        "  response = make_request(url, api_key)\n",
        "  tmp = pd.read_json(response.content, convert_dates=[\"t\"], date_unit=\"s\")\n",
        "  tmp.columns = [\"date\", name]\n",
        "  diff = tmp.columns.difference(metrics_df.columns)\n",
        "  metrics_df = pd.concat([metrics_df, tmp[diff]], axis=1, sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://api.glassnode.com/v1/metrics/indicators/average_dormancy?api_key=5a0cf8d7-d14a-44f4-b063-a76807cd5340&a=BTC&s=1279324800&u=1524960000\n",
            "https://api.glassnode.com/v1/metrics/indicators/velocity?api_key=5a0cf8d7-d14a-44f4-b063-a76807cd5340&a=BTC&s=1279324800&u=1524960000\n",
            "https://api.glassnode.com/v1/metrics/indicators/nvts?api_key=5a0cf8d7-d14a-44f4-b063-a76807cd5340&a=BTC&s=1279324800&u=1524960000\n",
            "https://api.glassnode.com/v1/metrics/indicators/sopr?api_key=5a0cf8d7-d14a-44f4-b063-a76807cd5340&a=BTC&s=1279324800&u=1524960000\n",
            "https://api.glassnode.com/v1/metrics/market/mvrv?api_key=5a0cf8d7-d14a-44f4-b063-a76807cd5340&a=BTC&s=1279324800&u=1524960000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za9CQ2Iyci_x",
        "colab_type": "text"
      },
      "source": [
        "### Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gL2gAUd7nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics_df[\"target_price\"] = price_df[\"values\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPmsY5WuZVak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics_df = metrics_df.set_index(\"date\", drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB2MwZG1Zd-b",
        "colab_type": "code",
        "outputId": "3466ec6c-d476-4b51-b630-fcbae4e55a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "metrics_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dormancy</th>\n",
              "      <th>velocity</th>\n",
              "      <th>nvts</th>\n",
              "      <th>sopr</th>\n",
              "      <th>mvrv</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-07-17</th>\n",
              "      <td>17.557391</td>\n",
              "      <td>0.009523</td>\n",
              "      <td>50.150020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.012190</td>\n",
              "      <td>0.085840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-18</th>\n",
              "      <td>37.898148</td>\n",
              "      <td>0.012957</td>\n",
              "      <td>65.319991</td>\n",
              "      <td>1.174760</td>\n",
              "      <td>0.015894</td>\n",
              "      <td>0.080800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-19</th>\n",
              "      <td>8.009993</td>\n",
              "      <td>0.016159</td>\n",
              "      <td>90.955940</td>\n",
              "      <td>1.318536</td>\n",
              "      <td>0.022353</td>\n",
              "      <td>0.074736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-20</th>\n",
              "      <td>2.325436</td>\n",
              "      <td>0.011241</td>\n",
              "      <td>79.371871</td>\n",
              "      <td>1.090517</td>\n",
              "      <td>0.019591</td>\n",
              "      <td>0.079193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-21</th>\n",
              "      <td>23.594425</td>\n",
              "      <td>0.010935</td>\n",
              "      <td>70.096419</td>\n",
              "      <td>1.065532</td>\n",
              "      <td>0.017317</td>\n",
              "      <td>0.058470</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             dormancy  velocity       nvts      sopr      mvrv  target_price\n",
              "date                                                                        \n",
              "2010-07-17  17.557391  0.009523  50.150020  1.000000  0.012190      0.085840\n",
              "2010-07-18  37.898148  0.012957  65.319991  1.174760  0.015894      0.080800\n",
              "2010-07-19   8.009993  0.016159  90.955940  1.318536  0.022353      0.074736\n",
              "2010-07-20   2.325436  0.011241  79.371871  1.090517  0.019591      0.079193\n",
              "2010-07-21  23.594425  0.010935  70.096419  1.065532  0.017317      0.058470"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 378
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8oyfoBaGSu",
        "colab_type": "text"
      },
      "source": [
        "### Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz8rUZ8DXGab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(df):\n",
        "  norm = (df - df.mean()) / (df.max() - df.min())\n",
        "  return norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkOSuPh9XY49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics_df = normalize(metrics_df) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mzhKYjIaKJa",
        "colab_type": "code",
        "outputId": "f3f64341-ff81-4e40-8fcc-4f8770bf6a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "metrics_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dormancy</th>\n",
              "      <th>velocity</th>\n",
              "      <th>nvts</th>\n",
              "      <th>sopr</th>\n",
              "      <th>mvrv</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-07-17</th>\n",
              "      <td>0.053215</td>\n",
              "      <td>-0.027421</td>\n",
              "      <td>0.193640</td>\n",
              "      <td>-0.010630</td>\n",
              "      <td>-0.229075</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-18</th>\n",
              "      <td>0.151102</td>\n",
              "      <td>-0.026649</td>\n",
              "      <td>0.282290</td>\n",
              "      <td>0.129321</td>\n",
              "      <td>-0.228566</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-19</th>\n",
              "      <td>0.007270</td>\n",
              "      <td>-0.025929</td>\n",
              "      <td>0.432100</td>\n",
              "      <td>0.244460</td>\n",
              "      <td>-0.227678</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-20</th>\n",
              "      <td>-0.020087</td>\n",
              "      <td>-0.027035</td>\n",
              "      <td>0.364405</td>\n",
              "      <td>0.061858</td>\n",
              "      <td>-0.228058</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-21</th>\n",
              "      <td>0.082268</td>\n",
              "      <td>-0.027104</td>\n",
              "      <td>0.310202</td>\n",
              "      <td>0.041849</td>\n",
              "      <td>-0.228371</td>\n",
              "      <td>-0.057379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            dormancy  velocity      nvts      sopr      mvrv  target_price\n",
              "date                                                                      \n",
              "2010-07-17  0.053215 -0.027421  0.193640 -0.010630 -0.229075     -0.057378\n",
              "2010-07-18  0.151102 -0.026649  0.282290  0.129321 -0.228566     -0.057378\n",
              "2010-07-19  0.007270 -0.025929  0.432100  0.244460 -0.227678     -0.057378\n",
              "2010-07-20 -0.020087 -0.027035  0.364405  0.061858 -0.228058     -0.057378\n",
              "2010-07-21  0.082268 -0.027104  0.310202  0.041849 -0.228371     -0.057379"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmG5s8F_eU8E",
        "colab_type": "text"
      },
      "source": [
        "### Test-Training split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9up_58LseZkl",
        "colab_type": "code",
        "outputId": "dbc3d1b9-c4d3-43ae-92bc-6881748dc860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_data, test_data = train_test_split(metrics_df, test_size=0.2, shuffle=False)\n",
        "print(f\"Training data size: {training_data.shape}, Testing data size: {test_data.shape}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size: (2274, 6), Testing data size: (569, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK0AlEkxkHfZ",
        "colab_type": "code",
        "outputId": "b0cd5a83-2d75-49e6-9919-8356f16432e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "training_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dormancy</th>\n",
              "      <th>velocity</th>\n",
              "      <th>nvts</th>\n",
              "      <th>sopr</th>\n",
              "      <th>mvrv</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-07-17</th>\n",
              "      <td>0.053215</td>\n",
              "      <td>-0.027421</td>\n",
              "      <td>0.193640</td>\n",
              "      <td>-0.010630</td>\n",
              "      <td>-0.229075</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-18</th>\n",
              "      <td>0.151102</td>\n",
              "      <td>-0.026649</td>\n",
              "      <td>0.282290</td>\n",
              "      <td>0.129321</td>\n",
              "      <td>-0.228566</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-19</th>\n",
              "      <td>0.007270</td>\n",
              "      <td>-0.025929</td>\n",
              "      <td>0.432100</td>\n",
              "      <td>0.244460</td>\n",
              "      <td>-0.227678</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-20</th>\n",
              "      <td>-0.020087</td>\n",
              "      <td>-0.027035</td>\n",
              "      <td>0.364405</td>\n",
              "      <td>0.061858</td>\n",
              "      <td>-0.228058</td>\n",
              "      <td>-0.057378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-07-21</th>\n",
              "      <td>0.082268</td>\n",
              "      <td>-0.027104</td>\n",
              "      <td>0.310202</td>\n",
              "      <td>0.041849</td>\n",
              "      <td>-0.228371</td>\n",
              "      <td>-0.057379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-10-02</th>\n",
              "      <td>-0.023238</td>\n",
              "      <td>-0.009494</td>\n",
              "      <td>-0.053498</td>\n",
              "      <td>-0.009859</td>\n",
              "      <td>-0.005268</td>\n",
              "      <td>-0.026192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-10-03</th>\n",
              "      <td>-0.012099</td>\n",
              "      <td>-0.001411</td>\n",
              "      <td>-0.053391</td>\n",
              "      <td>-0.006953</td>\n",
              "      <td>-0.005334</td>\n",
              "      <td>-0.026323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-10-04</th>\n",
              "      <td>0.000965</td>\n",
              "      <td>-0.002629</td>\n",
              "      <td>-0.053477</td>\n",
              "      <td>-0.007017</td>\n",
              "      <td>-0.006431</td>\n",
              "      <td>-0.026214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-10-05</th>\n",
              "      <td>-0.021508</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>-0.053226</td>\n",
              "      <td>-0.009309</td>\n",
              "      <td>-0.006179</td>\n",
              "      <td>-0.026234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-10-06</th>\n",
              "      <td>-0.021806</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>-0.053121</td>\n",
              "      <td>-0.009378</td>\n",
              "      <td>-0.005974</td>\n",
              "      <td>-0.025921</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2274 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            dormancy  velocity      nvts      sopr      mvrv  target_price\n",
              "date                                                                      \n",
              "2010-07-17  0.053215 -0.027421  0.193640 -0.010630 -0.229075     -0.057378\n",
              "2010-07-18  0.151102 -0.026649  0.282290  0.129321 -0.228566     -0.057378\n",
              "2010-07-19  0.007270 -0.025929  0.432100  0.244460 -0.227678     -0.057378\n",
              "2010-07-20 -0.020087 -0.027035  0.364405  0.061858 -0.228058     -0.057378\n",
              "2010-07-21  0.082268 -0.027104  0.310202  0.041849 -0.228371     -0.057379\n",
              "...              ...       ...       ...       ...       ...           ...\n",
              "2016-10-02 -0.023238 -0.009494 -0.053498 -0.009859 -0.005268     -0.026192\n",
              "2016-10-03 -0.012099 -0.001411 -0.053391 -0.006953 -0.005334     -0.026323\n",
              "2016-10-04  0.000965 -0.002629 -0.053477 -0.007017 -0.006431     -0.026214\n",
              "2016-10-05 -0.021508  0.002961 -0.053226 -0.009309 -0.006179     -0.026234\n",
              "2016-10-06 -0.021806  0.002800 -0.053121 -0.009378 -0.005974     -0.025921\n",
              "\n",
              "[2274 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBUCFLxbfCRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = training_data[[\"dormancy\", \"velocity\", \"nvts\", \"sopr\", \"mvrv\"]]\n",
        "y_train = training_data[\"target_price\"]\n",
        "\n",
        "x_test = test_data[[\"dormancy\", \"velocity\", \"nvts\", \"sopr\", \"mvrv\"]]\n",
        "y_test = test_data[\"target_price\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaD2kRO2TRcC",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWurkONeWCHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6g8jQIvW2aK",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqb-OAhuW3Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QrW_GzUh-Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1000\n",
        "DROPOUT = 0.2\n",
        "DIRECTIONS = 1\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 5\n",
        "OUTPUT_SIZE = 1\n",
        "SEQ_LENGTH = 90 # 90 day average\n",
        "NUM_FEATURES = 5\n",
        "HIDDEN_SIZE = 12\n",
        "LEARNING_RATE = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DNCrlW2W30n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWfVwbZMicz1",
        "colab_type": "text"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZveTUfuW-Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "    # self.dropout = nn.Dropout(prob_dropout)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x, states):\n",
        "    x, (h, c) = self.lstm(x, states)\n",
        "    out = self.linear(x)\n",
        "    return out, (h, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjenlKh1jsiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM(\n",
        "    NUM_FEATURES,\n",
        "    HIDDEN_SIZE,\n",
        "    NUM_LAYERS,\n",
        "    OUTPUT_SIZE,\n",
        "    DROPOUT\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.linear.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX1R_NiIjYfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_batches = len(x_train) // BATCH_SIZE // SEQ_LENGTH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s1HpwdXjgO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model, epochs, state_dim):\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      # Initialize states\n",
        "      # (num_layers * num_directions, batch, hidden_size)\n",
        "      states = (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n",
        "\n",
        "      # Begin training\n",
        "      for idx in range(num_batches):\n",
        "        \n",
        "          # Define range\n",
        "          start = idx\n",
        "          end = idx + SEQ_LENGTH * BATCH_SIZE\n",
        "          x_window = x_train[start:end]\n",
        "          y_window = y_train[start:end]\n",
        "\n",
        "          # Convert to Tensors and modify dimensions\n",
        "          x_tensor = torch.tensor(x_window.values).float()\n",
        "          y_tensor = torch.tensor(y_window.values).float()\n",
        "          x_batch = x_tensor.view(BATCH_SIZE, SEQ_LENGTH, NUM_FEATURES)\n",
        "          y_batch = y_tensor.view(BATCH_SIZE, SEQ_LENGTH).unsqueeze(dim=2)\n",
        "        \n",
        "          # Move to GPU\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          prediction, states = model(x_batch, states)\n",
        "\n",
        "          # Detach states\n",
        "          states = [state.detach() for state in states]\n",
        "          \n",
        "          loss = criterion(prediction, y_batch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          print('Epoch [{}/{}], Index: [{}], Loss: {:.4f}'\n",
        "                .format(epoch+1, epochs, idx + 1, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sog7ScrUjwTR",
        "colab_type": "code",
        "outputId": "274746ba-09d2-4df4-fafb-80c197b0642d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training(model, EPOCHS, state_dim=(NUM_LAYERS * DIRECTIONS, BATCH_SIZE, HIDDEN_SIZE))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Index: [1], Loss: 0.0391\n",
            "Epoch [1/1000], Index: [2], Loss: 0.0391\n",
            "Epoch [1/1000], Index: [3], Loss: 0.0390\n",
            "Epoch [1/1000], Index: [4], Loss: 0.0389\n",
            "Epoch [1/1000], Index: [5], Loss: 0.0390\n",
            "Epoch [2/1000], Index: [1], Loss: 0.0386\n",
            "Epoch [2/1000], Index: [2], Loss: 0.0388\n",
            "Epoch [2/1000], Index: [3], Loss: 0.0385\n",
            "Epoch [2/1000], Index: [4], Loss: 0.0387\n",
            "Epoch [2/1000], Index: [5], Loss: 0.0388\n",
            "Epoch [3/1000], Index: [1], Loss: 0.0386\n",
            "Epoch [3/1000], Index: [2], Loss: 0.0384\n",
            "Epoch [3/1000], Index: [3], Loss: 0.0386\n",
            "Epoch [3/1000], Index: [4], Loss: 0.0385\n",
            "Epoch [3/1000], Index: [5], Loss: 0.0382\n",
            "Epoch [4/1000], Index: [1], Loss: 0.0379\n",
            "Epoch [4/1000], Index: [2], Loss: 0.0381\n",
            "Epoch [4/1000], Index: [3], Loss: 0.0382\n",
            "Epoch [4/1000], Index: [4], Loss: 0.0380\n",
            "Epoch [4/1000], Index: [5], Loss: 0.0378\n",
            "Epoch [5/1000], Index: [1], Loss: 0.0378\n",
            "Epoch [5/1000], Index: [2], Loss: 0.0380\n",
            "Epoch [5/1000], Index: [3], Loss: 0.0378\n",
            "Epoch [5/1000], Index: [4], Loss: 0.0377\n",
            "Epoch [5/1000], Index: [5], Loss: 0.0376\n",
            "Epoch [6/1000], Index: [1], Loss: 0.0376\n",
            "Epoch [6/1000], Index: [2], Loss: 0.0376\n",
            "Epoch [6/1000], Index: [3], Loss: 0.0373\n",
            "Epoch [6/1000], Index: [4], Loss: 0.0371\n",
            "Epoch [6/1000], Index: [5], Loss: 0.0373\n",
            "Epoch [7/1000], Index: [1], Loss: 0.0370\n",
            "Epoch [7/1000], Index: [2], Loss: 0.0372\n",
            "Epoch [7/1000], Index: [3], Loss: 0.0371\n",
            "Epoch [7/1000], Index: [4], Loss: 0.0371\n",
            "Epoch [7/1000], Index: [5], Loss: 0.0370\n",
            "Epoch [8/1000], Index: [1], Loss: 0.0368\n",
            "Epoch [8/1000], Index: [2], Loss: 0.0369\n",
            "Epoch [8/1000], Index: [3], Loss: 0.0368\n",
            "Epoch [8/1000], Index: [4], Loss: 0.0367\n",
            "Epoch [8/1000], Index: [5], Loss: 0.0368\n",
            "Epoch [9/1000], Index: [1], Loss: 0.0364\n",
            "Epoch [9/1000], Index: [2], Loss: 0.0362\n",
            "Epoch [9/1000], Index: [3], Loss: 0.0366\n",
            "Epoch [9/1000], Index: [4], Loss: 0.0363\n",
            "Epoch [9/1000], Index: [5], Loss: 0.0363\n",
            "Epoch [10/1000], Index: [1], Loss: 0.0360\n",
            "Epoch [10/1000], Index: [2], Loss: 0.0363\n",
            "Epoch [10/1000], Index: [3], Loss: 0.0363\n",
            "Epoch [10/1000], Index: [4], Loss: 0.0360\n",
            "Epoch [10/1000], Index: [5], Loss: 0.0362\n",
            "Epoch [11/1000], Index: [1], Loss: 0.0358\n",
            "Epoch [11/1000], Index: [2], Loss: 0.0359\n",
            "Epoch [11/1000], Index: [3], Loss: 0.0358\n",
            "Epoch [11/1000], Index: [4], Loss: 0.0359\n",
            "Epoch [11/1000], Index: [5], Loss: 0.0357\n",
            "Epoch [12/1000], Index: [1], Loss: 0.0355\n",
            "Epoch [12/1000], Index: [2], Loss: 0.0356\n",
            "Epoch [12/1000], Index: [3], Loss: 0.0357\n",
            "Epoch [12/1000], Index: [4], Loss: 0.0353\n",
            "Epoch [12/1000], Index: [5], Loss: 0.0353\n",
            "Epoch [13/1000], Index: [1], Loss: 0.0353\n",
            "Epoch [13/1000], Index: [2], Loss: 0.0353\n",
            "Epoch [13/1000], Index: [3], Loss: 0.0354\n",
            "Epoch [13/1000], Index: [4], Loss: 0.0352\n",
            "Epoch [13/1000], Index: [5], Loss: 0.0351\n",
            "Epoch [14/1000], Index: [1], Loss: 0.0347\n",
            "Epoch [14/1000], Index: [2], Loss: 0.0351\n",
            "Epoch [14/1000], Index: [3], Loss: 0.0349\n",
            "Epoch [14/1000], Index: [4], Loss: 0.0349\n",
            "Epoch [14/1000], Index: [5], Loss: 0.0350\n",
            "Epoch [15/1000], Index: [1], Loss: 0.0345\n",
            "Epoch [15/1000], Index: [2], Loss: 0.0349\n",
            "Epoch [15/1000], Index: [3], Loss: 0.0347\n",
            "Epoch [15/1000], Index: [4], Loss: 0.0346\n",
            "Epoch [15/1000], Index: [5], Loss: 0.0343\n",
            "Epoch [16/1000], Index: [1], Loss: 0.0346\n",
            "Epoch [16/1000], Index: [2], Loss: 0.0343\n",
            "Epoch [16/1000], Index: [3], Loss: 0.0345\n",
            "Epoch [16/1000], Index: [4], Loss: 0.0345\n",
            "Epoch [16/1000], Index: [5], Loss: 0.0343\n",
            "Epoch [17/1000], Index: [1], Loss: 0.0340\n",
            "Epoch [17/1000], Index: [2], Loss: 0.0341\n",
            "Epoch [17/1000], Index: [3], Loss: 0.0339\n",
            "Epoch [17/1000], Index: [4], Loss: 0.0340\n",
            "Epoch [17/1000], Index: [5], Loss: 0.0338\n",
            "Epoch [18/1000], Index: [1], Loss: 0.0335\n",
            "Epoch [18/1000], Index: [2], Loss: 0.0337\n",
            "Epoch [18/1000], Index: [3], Loss: 0.0337\n",
            "Epoch [18/1000], Index: [4], Loss: 0.0336\n",
            "Epoch [18/1000], Index: [5], Loss: 0.0335\n",
            "Epoch [19/1000], Index: [1], Loss: 0.0333\n",
            "Epoch [19/1000], Index: [2], Loss: 0.0335\n",
            "Epoch [19/1000], Index: [3], Loss: 0.0337\n",
            "Epoch [19/1000], Index: [4], Loss: 0.0333\n",
            "Epoch [19/1000], Index: [5], Loss: 0.0332\n",
            "Epoch [20/1000], Index: [1], Loss: 0.0331\n",
            "Epoch [20/1000], Index: [2], Loss: 0.0333\n",
            "Epoch [20/1000], Index: [3], Loss: 0.0331\n",
            "Epoch [20/1000], Index: [4], Loss: 0.0331\n",
            "Epoch [20/1000], Index: [5], Loss: 0.0330\n",
            "Epoch [21/1000], Index: [1], Loss: 0.0328\n",
            "Epoch [21/1000], Index: [2], Loss: 0.0329\n",
            "Epoch [21/1000], Index: [3], Loss: 0.0326\n",
            "Epoch [21/1000], Index: [4], Loss: 0.0329\n",
            "Epoch [21/1000], Index: [5], Loss: 0.0329\n",
            "Epoch [22/1000], Index: [1], Loss: 0.0325\n",
            "Epoch [22/1000], Index: [2], Loss: 0.0324\n",
            "Epoch [22/1000], Index: [3], Loss: 0.0324\n",
            "Epoch [22/1000], Index: [4], Loss: 0.0326\n",
            "Epoch [22/1000], Index: [5], Loss: 0.0324\n",
            "Epoch [23/1000], Index: [1], Loss: 0.0322\n",
            "Epoch [23/1000], Index: [2], Loss: 0.0322\n",
            "Epoch [23/1000], Index: [3], Loss: 0.0323\n",
            "Epoch [23/1000], Index: [4], Loss: 0.0322\n",
            "Epoch [23/1000], Index: [5], Loss: 0.0322\n",
            "Epoch [24/1000], Index: [1], Loss: 0.0320\n",
            "Epoch [24/1000], Index: [2], Loss: 0.0318\n",
            "Epoch [24/1000], Index: [3], Loss: 0.0319\n",
            "Epoch [24/1000], Index: [4], Loss: 0.0319\n",
            "Epoch [24/1000], Index: [5], Loss: 0.0318\n",
            "Epoch [25/1000], Index: [1], Loss: 0.0316\n",
            "Epoch [25/1000], Index: [2], Loss: 0.0318\n",
            "Epoch [25/1000], Index: [3], Loss: 0.0316\n",
            "Epoch [25/1000], Index: [4], Loss: 0.0317\n",
            "Epoch [25/1000], Index: [5], Loss: 0.0317\n",
            "Epoch [26/1000], Index: [1], Loss: 0.0313\n",
            "Epoch [26/1000], Index: [2], Loss: 0.0315\n",
            "Epoch [26/1000], Index: [3], Loss: 0.0316\n",
            "Epoch [26/1000], Index: [4], Loss: 0.0313\n",
            "Epoch [26/1000], Index: [5], Loss: 0.0314\n",
            "Epoch [27/1000], Index: [1], Loss: 0.0312\n",
            "Epoch [27/1000], Index: [2], Loss: 0.0311\n",
            "Epoch [27/1000], Index: [3], Loss: 0.0310\n",
            "Epoch [27/1000], Index: [4], Loss: 0.0310\n",
            "Epoch [27/1000], Index: [5], Loss: 0.0311\n",
            "Epoch [28/1000], Index: [1], Loss: 0.0307\n",
            "Epoch [28/1000], Index: [2], Loss: 0.0310\n",
            "Epoch [28/1000], Index: [3], Loss: 0.0310\n",
            "Epoch [28/1000], Index: [4], Loss: 0.0307\n",
            "Epoch [28/1000], Index: [5], Loss: 0.0308\n",
            "Epoch [29/1000], Index: [1], Loss: 0.0304\n",
            "Epoch [29/1000], Index: [2], Loss: 0.0305\n",
            "Epoch [29/1000], Index: [3], Loss: 0.0305\n",
            "Epoch [29/1000], Index: [4], Loss: 0.0303\n",
            "Epoch [29/1000], Index: [5], Loss: 0.0304\n",
            "Epoch [30/1000], Index: [1], Loss: 0.0303\n",
            "Epoch [30/1000], Index: [2], Loss: 0.0303\n",
            "Epoch [30/1000], Index: [3], Loss: 0.0303\n",
            "Epoch [30/1000], Index: [4], Loss: 0.0304\n",
            "Epoch [30/1000], Index: [5], Loss: 0.0301\n",
            "Epoch [31/1000], Index: [1], Loss: 0.0300\n",
            "Epoch [31/1000], Index: [2], Loss: 0.0301\n",
            "Epoch [31/1000], Index: [3], Loss: 0.0302\n",
            "Epoch [31/1000], Index: [4], Loss: 0.0300\n",
            "Epoch [31/1000], Index: [5], Loss: 0.0297\n",
            "Epoch [32/1000], Index: [1], Loss: 0.0298\n",
            "Epoch [32/1000], Index: [2], Loss: 0.0300\n",
            "Epoch [32/1000], Index: [3], Loss: 0.0299\n",
            "Epoch [32/1000], Index: [4], Loss: 0.0296\n",
            "Epoch [32/1000], Index: [5], Loss: 0.0298\n",
            "Epoch [33/1000], Index: [1], Loss: 0.0294\n",
            "Epoch [33/1000], Index: [2], Loss: 0.0295\n",
            "Epoch [33/1000], Index: [3], Loss: 0.0293\n",
            "Epoch [33/1000], Index: [4], Loss: 0.0294\n",
            "Epoch [33/1000], Index: [5], Loss: 0.0294\n",
            "Epoch [34/1000], Index: [1], Loss: 0.0293\n",
            "Epoch [34/1000], Index: [2], Loss: 0.0292\n",
            "Epoch [34/1000], Index: [3], Loss: 0.0291\n",
            "Epoch [34/1000], Index: [4], Loss: 0.0291\n",
            "Epoch [34/1000], Index: [5], Loss: 0.0293\n",
            "Epoch [35/1000], Index: [1], Loss: 0.0286\n",
            "Epoch [35/1000], Index: [2], Loss: 0.0290\n",
            "Epoch [35/1000], Index: [3], Loss: 0.0290\n",
            "Epoch [35/1000], Index: [4], Loss: 0.0287\n",
            "Epoch [35/1000], Index: [5], Loss: 0.0289\n",
            "Epoch [36/1000], Index: [1], Loss: 0.0287\n",
            "Epoch [36/1000], Index: [2], Loss: 0.0289\n",
            "Epoch [36/1000], Index: [3], Loss: 0.0287\n",
            "Epoch [36/1000], Index: [4], Loss: 0.0287\n",
            "Epoch [36/1000], Index: [5], Loss: 0.0286\n",
            "Epoch [37/1000], Index: [1], Loss: 0.0283\n",
            "Epoch [37/1000], Index: [2], Loss: 0.0284\n",
            "Epoch [37/1000], Index: [3], Loss: 0.0284\n",
            "Epoch [37/1000], Index: [4], Loss: 0.0283\n",
            "Epoch [37/1000], Index: [5], Loss: 0.0283\n",
            "Epoch [38/1000], Index: [1], Loss: 0.0282\n",
            "Epoch [38/1000], Index: [2], Loss: 0.0282\n",
            "Epoch [38/1000], Index: [3], Loss: 0.0282\n",
            "Epoch [38/1000], Index: [4], Loss: 0.0281\n",
            "Epoch [38/1000], Index: [5], Loss: 0.0281\n",
            "Epoch [39/1000], Index: [1], Loss: 0.0280\n",
            "Epoch [39/1000], Index: [2], Loss: 0.0278\n",
            "Epoch [39/1000], Index: [3], Loss: 0.0279\n",
            "Epoch [39/1000], Index: [4], Loss: 0.0279\n",
            "Epoch [39/1000], Index: [5], Loss: 0.0279\n",
            "Epoch [40/1000], Index: [1], Loss: 0.0276\n",
            "Epoch [40/1000], Index: [2], Loss: 0.0276\n",
            "Epoch [40/1000], Index: [3], Loss: 0.0276\n",
            "Epoch [40/1000], Index: [4], Loss: 0.0277\n",
            "Epoch [40/1000], Index: [5], Loss: 0.0276\n",
            "Epoch [41/1000], Index: [1], Loss: 0.0274\n",
            "Epoch [41/1000], Index: [2], Loss: 0.0274\n",
            "Epoch [41/1000], Index: [3], Loss: 0.0273\n",
            "Epoch [41/1000], Index: [4], Loss: 0.0273\n",
            "Epoch [41/1000], Index: [5], Loss: 0.0271\n",
            "Epoch [42/1000], Index: [1], Loss: 0.0272\n",
            "Epoch [42/1000], Index: [2], Loss: 0.0272\n",
            "Epoch [42/1000], Index: [3], Loss: 0.0273\n",
            "Epoch [42/1000], Index: [4], Loss: 0.0271\n",
            "Epoch [42/1000], Index: [5], Loss: 0.0269\n",
            "Epoch [43/1000], Index: [1], Loss: 0.0270\n",
            "Epoch [43/1000], Index: [2], Loss: 0.0270\n",
            "Epoch [43/1000], Index: [3], Loss: 0.0267\n",
            "Epoch [43/1000], Index: [4], Loss: 0.0269\n",
            "Epoch [43/1000], Index: [5], Loss: 0.0269\n",
            "Epoch [44/1000], Index: [1], Loss: 0.0266\n",
            "Epoch [44/1000], Index: [2], Loss: 0.0267\n",
            "Epoch [44/1000], Index: [3], Loss: 0.0268\n",
            "Epoch [44/1000], Index: [4], Loss: 0.0266\n",
            "Epoch [44/1000], Index: [5], Loss: 0.0265\n",
            "Epoch [45/1000], Index: [1], Loss: 0.0264\n",
            "Epoch [45/1000], Index: [2], Loss: 0.0262\n",
            "Epoch [45/1000], Index: [3], Loss: 0.0265\n",
            "Epoch [45/1000], Index: [4], Loss: 0.0262\n",
            "Epoch [45/1000], Index: [5], Loss: 0.0263\n",
            "Epoch [46/1000], Index: [1], Loss: 0.0262\n",
            "Epoch [46/1000], Index: [2], Loss: 0.0262\n",
            "Epoch [46/1000], Index: [3], Loss: 0.0260\n",
            "Epoch [46/1000], Index: [4], Loss: 0.0260\n",
            "Epoch [46/1000], Index: [5], Loss: 0.0261\n",
            "Epoch [47/1000], Index: [1], Loss: 0.0259\n",
            "Epoch [47/1000], Index: [2], Loss: 0.0260\n",
            "Epoch [47/1000], Index: [3], Loss: 0.0258\n",
            "Epoch [47/1000], Index: [4], Loss: 0.0259\n",
            "Epoch [47/1000], Index: [5], Loss: 0.0259\n",
            "Epoch [48/1000], Index: [1], Loss: 0.0256\n",
            "Epoch [48/1000], Index: [2], Loss: 0.0257\n",
            "Epoch [48/1000], Index: [3], Loss: 0.0257\n",
            "Epoch [48/1000], Index: [4], Loss: 0.0256\n",
            "Epoch [48/1000], Index: [5], Loss: 0.0255\n",
            "Epoch [49/1000], Index: [1], Loss: 0.0253\n",
            "Epoch [49/1000], Index: [2], Loss: 0.0255\n",
            "Epoch [49/1000], Index: [3], Loss: 0.0254\n",
            "Epoch [49/1000], Index: [4], Loss: 0.0252\n",
            "Epoch [49/1000], Index: [5], Loss: 0.0253\n",
            "Epoch [50/1000], Index: [1], Loss: 0.0252\n",
            "Epoch [50/1000], Index: [2], Loss: 0.0253\n",
            "Epoch [50/1000], Index: [3], Loss: 0.0251\n",
            "Epoch [50/1000], Index: [4], Loss: 0.0251\n",
            "Epoch [50/1000], Index: [5], Loss: 0.0250\n",
            "Epoch [51/1000], Index: [1], Loss: 0.0248\n",
            "Epoch [51/1000], Index: [2], Loss: 0.0250\n",
            "Epoch [51/1000], Index: [3], Loss: 0.0248\n",
            "Epoch [51/1000], Index: [4], Loss: 0.0250\n",
            "Epoch [51/1000], Index: [5], Loss: 0.0248\n",
            "Epoch [52/1000], Index: [1], Loss: 0.0247\n",
            "Epoch [52/1000], Index: [2], Loss: 0.0246\n",
            "Epoch [52/1000], Index: [3], Loss: 0.0246\n",
            "Epoch [52/1000], Index: [4], Loss: 0.0246\n",
            "Epoch [52/1000], Index: [5], Loss: 0.0246\n",
            "Epoch [53/1000], Index: [1], Loss: 0.0244\n",
            "Epoch [53/1000], Index: [2], Loss: 0.0244\n",
            "Epoch [53/1000], Index: [3], Loss: 0.0242\n",
            "Epoch [53/1000], Index: [4], Loss: 0.0244\n",
            "Epoch [53/1000], Index: [5], Loss: 0.0244\n",
            "Epoch [54/1000], Index: [1], Loss: 0.0242\n",
            "Epoch [54/1000], Index: [2], Loss: 0.0242\n",
            "Epoch [54/1000], Index: [3], Loss: 0.0241\n",
            "Epoch [54/1000], Index: [4], Loss: 0.0241\n",
            "Epoch [54/1000], Index: [5], Loss: 0.0242\n",
            "Epoch [55/1000], Index: [1], Loss: 0.0240\n",
            "Epoch [55/1000], Index: [2], Loss: 0.0241\n",
            "Epoch [55/1000], Index: [3], Loss: 0.0239\n",
            "Epoch [55/1000], Index: [4], Loss: 0.0240\n",
            "Epoch [55/1000], Index: [5], Loss: 0.0239\n",
            "Epoch [56/1000], Index: [1], Loss: 0.0238\n",
            "Epoch [56/1000], Index: [2], Loss: 0.0238\n",
            "Epoch [56/1000], Index: [3], Loss: 0.0239\n",
            "Epoch [56/1000], Index: [4], Loss: 0.0238\n",
            "Epoch [56/1000], Index: [5], Loss: 0.0237\n",
            "Epoch [57/1000], Index: [1], Loss: 0.0236\n",
            "Epoch [57/1000], Index: [2], Loss: 0.0235\n",
            "Epoch [57/1000], Index: [3], Loss: 0.0235\n",
            "Epoch [57/1000], Index: [4], Loss: 0.0235\n",
            "Epoch [57/1000], Index: [5], Loss: 0.0234\n",
            "Epoch [58/1000], Index: [1], Loss: 0.0234\n",
            "Epoch [58/1000], Index: [2], Loss: 0.0234\n",
            "Epoch [58/1000], Index: [3], Loss: 0.0234\n",
            "Epoch [58/1000], Index: [4], Loss: 0.0233\n",
            "Epoch [58/1000], Index: [5], Loss: 0.0232\n",
            "Epoch [59/1000], Index: [1], Loss: 0.0232\n",
            "Epoch [59/1000], Index: [2], Loss: 0.0230\n",
            "Epoch [59/1000], Index: [3], Loss: 0.0231\n",
            "Epoch [59/1000], Index: [4], Loss: 0.0230\n",
            "Epoch [59/1000], Index: [5], Loss: 0.0230\n",
            "Epoch [60/1000], Index: [1], Loss: 0.0229\n",
            "Epoch [60/1000], Index: [2], Loss: 0.0229\n",
            "Epoch [60/1000], Index: [3], Loss: 0.0228\n",
            "Epoch [60/1000], Index: [4], Loss: 0.0228\n",
            "Epoch [60/1000], Index: [5], Loss: 0.0228\n",
            "Epoch [61/1000], Index: [1], Loss: 0.0227\n",
            "Epoch [61/1000], Index: [2], Loss: 0.0227\n",
            "Epoch [61/1000], Index: [3], Loss: 0.0226\n",
            "Epoch [61/1000], Index: [4], Loss: 0.0226\n",
            "Epoch [61/1000], Index: [5], Loss: 0.0226\n",
            "Epoch [62/1000], Index: [1], Loss: 0.0223\n",
            "Epoch [62/1000], Index: [2], Loss: 0.0222\n",
            "Epoch [62/1000], Index: [3], Loss: 0.0224\n",
            "Epoch [62/1000], Index: [4], Loss: 0.0223\n",
            "Epoch [62/1000], Index: [5], Loss: 0.0222\n",
            "Epoch [63/1000], Index: [1], Loss: 0.0222\n",
            "Epoch [63/1000], Index: [2], Loss: 0.0222\n",
            "Epoch [63/1000], Index: [3], Loss: 0.0222\n",
            "Epoch [63/1000], Index: [4], Loss: 0.0220\n",
            "Epoch [63/1000], Index: [5], Loss: 0.0221\n",
            "Epoch [64/1000], Index: [1], Loss: 0.0219\n",
            "Epoch [64/1000], Index: [2], Loss: 0.0220\n",
            "Epoch [64/1000], Index: [3], Loss: 0.0220\n",
            "Epoch [64/1000], Index: [4], Loss: 0.0219\n",
            "Epoch [64/1000], Index: [5], Loss: 0.0219\n",
            "Epoch [65/1000], Index: [1], Loss: 0.0218\n",
            "Epoch [65/1000], Index: [2], Loss: 0.0217\n",
            "Epoch [65/1000], Index: [3], Loss: 0.0219\n",
            "Epoch [65/1000], Index: [4], Loss: 0.0216\n",
            "Epoch [65/1000], Index: [5], Loss: 0.0216\n",
            "Epoch [66/1000], Index: [1], Loss: 0.0213\n",
            "Epoch [66/1000], Index: [2], Loss: 0.0217\n",
            "Epoch [66/1000], Index: [3], Loss: 0.0215\n",
            "Epoch [66/1000], Index: [4], Loss: 0.0216\n",
            "Epoch [66/1000], Index: [5], Loss: 0.0212\n",
            "Epoch [67/1000], Index: [1], Loss: 0.0212\n",
            "Epoch [67/1000], Index: [2], Loss: 0.0212\n",
            "Epoch [67/1000], Index: [3], Loss: 0.0214\n",
            "Epoch [67/1000], Index: [4], Loss: 0.0213\n",
            "Epoch [67/1000], Index: [5], Loss: 0.0214\n",
            "Epoch [68/1000], Index: [1], Loss: 0.0213\n",
            "Epoch [68/1000], Index: [2], Loss: 0.0212\n",
            "Epoch [68/1000], Index: [3], Loss: 0.0212\n",
            "Epoch [68/1000], Index: [4], Loss: 0.0211\n",
            "Epoch [68/1000], Index: [5], Loss: 0.0210\n",
            "Epoch [69/1000], Index: [1], Loss: 0.0210\n",
            "Epoch [69/1000], Index: [2], Loss: 0.0209\n",
            "Epoch [69/1000], Index: [3], Loss: 0.0210\n",
            "Epoch [69/1000], Index: [4], Loss: 0.0210\n",
            "Epoch [69/1000], Index: [5], Loss: 0.0208\n",
            "Epoch [70/1000], Index: [1], Loss: 0.0207\n",
            "Epoch [70/1000], Index: [2], Loss: 0.0208\n",
            "Epoch [70/1000], Index: [3], Loss: 0.0207\n",
            "Epoch [70/1000], Index: [4], Loss: 0.0206\n",
            "Epoch [70/1000], Index: [5], Loss: 0.0206\n",
            "Epoch [71/1000], Index: [1], Loss: 0.0206\n",
            "Epoch [71/1000], Index: [2], Loss: 0.0204\n",
            "Epoch [71/1000], Index: [3], Loss: 0.0206\n",
            "Epoch [71/1000], Index: [4], Loss: 0.0203\n",
            "Epoch [71/1000], Index: [5], Loss: 0.0205\n",
            "Epoch [72/1000], Index: [1], Loss: 0.0204\n",
            "Epoch [72/1000], Index: [2], Loss: 0.0205\n",
            "Epoch [72/1000], Index: [3], Loss: 0.0202\n",
            "Epoch [72/1000], Index: [4], Loss: 0.0203\n",
            "Epoch [72/1000], Index: [5], Loss: 0.0202\n",
            "Epoch [73/1000], Index: [1], Loss: 0.0202\n",
            "Epoch [73/1000], Index: [2], Loss: 0.0202\n",
            "Epoch [73/1000], Index: [3], Loss: 0.0201\n",
            "Epoch [73/1000], Index: [4], Loss: 0.0200\n",
            "Epoch [73/1000], Index: [5], Loss: 0.0200\n",
            "Epoch [74/1000], Index: [1], Loss: 0.0199\n",
            "Epoch [74/1000], Index: [2], Loss: 0.0200\n",
            "Epoch [74/1000], Index: [3], Loss: 0.0200\n",
            "Epoch [74/1000], Index: [4], Loss: 0.0198\n",
            "Epoch [74/1000], Index: [5], Loss: 0.0199\n",
            "Epoch [75/1000], Index: [1], Loss: 0.0198\n",
            "Epoch [75/1000], Index: [2], Loss: 0.0197\n",
            "Epoch [75/1000], Index: [3], Loss: 0.0197\n",
            "Epoch [75/1000], Index: [4], Loss: 0.0197\n",
            "Epoch [75/1000], Index: [5], Loss: 0.0195\n",
            "Epoch [76/1000], Index: [1], Loss: 0.0195\n",
            "Epoch [76/1000], Index: [2], Loss: 0.0195\n",
            "Epoch [76/1000], Index: [3], Loss: 0.0196\n",
            "Epoch [76/1000], Index: [4], Loss: 0.0195\n",
            "Epoch [76/1000], Index: [5], Loss: 0.0195\n",
            "Epoch [77/1000], Index: [1], Loss: 0.0194\n",
            "Epoch [77/1000], Index: [2], Loss: 0.0194\n",
            "Epoch [77/1000], Index: [3], Loss: 0.0194\n",
            "Epoch [77/1000], Index: [4], Loss: 0.0193\n",
            "Epoch [77/1000], Index: [5], Loss: 0.0192\n",
            "Epoch [78/1000], Index: [1], Loss: 0.0190\n",
            "Epoch [78/1000], Index: [2], Loss: 0.0192\n",
            "Epoch [78/1000], Index: [3], Loss: 0.0192\n",
            "Epoch [78/1000], Index: [4], Loss: 0.0191\n",
            "Epoch [78/1000], Index: [5], Loss: 0.0191\n",
            "Epoch [79/1000], Index: [1], Loss: 0.0189\n",
            "Epoch [79/1000], Index: [2], Loss: 0.0190\n",
            "Epoch [79/1000], Index: [3], Loss: 0.0190\n",
            "Epoch [79/1000], Index: [4], Loss: 0.0189\n",
            "Epoch [79/1000], Index: [5], Loss: 0.0188\n",
            "Epoch [80/1000], Index: [1], Loss: 0.0189\n",
            "Epoch [80/1000], Index: [2], Loss: 0.0187\n",
            "Epoch [80/1000], Index: [3], Loss: 0.0187\n",
            "Epoch [80/1000], Index: [4], Loss: 0.0187\n",
            "Epoch [80/1000], Index: [5], Loss: 0.0187\n",
            "Epoch [81/1000], Index: [1], Loss: 0.0185\n",
            "Epoch [81/1000], Index: [2], Loss: 0.0185\n",
            "Epoch [81/1000], Index: [3], Loss: 0.0185\n",
            "Epoch [81/1000], Index: [4], Loss: 0.0184\n",
            "Epoch [81/1000], Index: [5], Loss: 0.0186\n",
            "Epoch [82/1000], Index: [1], Loss: 0.0184\n",
            "Epoch [82/1000], Index: [2], Loss: 0.0184\n",
            "Epoch [82/1000], Index: [3], Loss: 0.0183\n",
            "Epoch [82/1000], Index: [4], Loss: 0.0184\n",
            "Epoch [82/1000], Index: [5], Loss: 0.0182\n",
            "Epoch [83/1000], Index: [1], Loss: 0.0181\n",
            "Epoch [83/1000], Index: [2], Loss: 0.0182\n",
            "Epoch [83/1000], Index: [3], Loss: 0.0183\n",
            "Epoch [83/1000], Index: [4], Loss: 0.0181\n",
            "Epoch [83/1000], Index: [5], Loss: 0.0180\n",
            "Epoch [84/1000], Index: [1], Loss: 0.0180\n",
            "Epoch [84/1000], Index: [2], Loss: 0.0180\n",
            "Epoch [84/1000], Index: [3], Loss: 0.0180\n",
            "Epoch [84/1000], Index: [4], Loss: 0.0180\n",
            "Epoch [84/1000], Index: [5], Loss: 0.0179\n",
            "Epoch [85/1000], Index: [1], Loss: 0.0178\n",
            "Epoch [85/1000], Index: [2], Loss: 0.0178\n",
            "Epoch [85/1000], Index: [3], Loss: 0.0178\n",
            "Epoch [85/1000], Index: [4], Loss: 0.0178\n",
            "Epoch [85/1000], Index: [5], Loss: 0.0177\n",
            "Epoch [86/1000], Index: [1], Loss: 0.0178\n",
            "Epoch [86/1000], Index: [2], Loss: 0.0177\n",
            "Epoch [86/1000], Index: [3], Loss: 0.0177\n",
            "Epoch [86/1000], Index: [4], Loss: 0.0176\n",
            "Epoch [86/1000], Index: [5], Loss: 0.0175\n",
            "Epoch [87/1000], Index: [1], Loss: 0.0175\n",
            "Epoch [87/1000], Index: [2], Loss: 0.0175\n",
            "Epoch [87/1000], Index: [3], Loss: 0.0175\n",
            "Epoch [87/1000], Index: [4], Loss: 0.0174\n",
            "Epoch [87/1000], Index: [5], Loss: 0.0174\n",
            "Epoch [88/1000], Index: [1], Loss: 0.0174\n",
            "Epoch [88/1000], Index: [2], Loss: 0.0172\n",
            "Epoch [88/1000], Index: [3], Loss: 0.0174\n",
            "Epoch [88/1000], Index: [4], Loss: 0.0172\n",
            "Epoch [88/1000], Index: [5], Loss: 0.0171\n",
            "Epoch [89/1000], Index: [1], Loss: 0.0171\n",
            "Epoch [89/1000], Index: [2], Loss: 0.0171\n",
            "Epoch [89/1000], Index: [3], Loss: 0.0170\n",
            "Epoch [89/1000], Index: [4], Loss: 0.0170\n",
            "Epoch [89/1000], Index: [5], Loss: 0.0169\n",
            "Epoch [90/1000], Index: [1], Loss: 0.0169\n",
            "Epoch [90/1000], Index: [2], Loss: 0.0168\n",
            "Epoch [90/1000], Index: [3], Loss: 0.0169\n",
            "Epoch [90/1000], Index: [4], Loss: 0.0169\n",
            "Epoch [90/1000], Index: [5], Loss: 0.0167\n",
            "Epoch [91/1000], Index: [1], Loss: 0.0166\n",
            "Epoch [91/1000], Index: [2], Loss: 0.0168\n",
            "Epoch [91/1000], Index: [3], Loss: 0.0166\n",
            "Epoch [91/1000], Index: [4], Loss: 0.0167\n",
            "Epoch [91/1000], Index: [5], Loss: 0.0166\n",
            "Epoch [92/1000], Index: [1], Loss: 0.0166\n",
            "Epoch [92/1000], Index: [2], Loss: 0.0165\n",
            "Epoch [92/1000], Index: [3], Loss: 0.0165\n",
            "Epoch [92/1000], Index: [4], Loss: 0.0166\n",
            "Epoch [92/1000], Index: [5], Loss: 0.0165\n",
            "Epoch [93/1000], Index: [1], Loss: 0.0164\n",
            "Epoch [93/1000], Index: [2], Loss: 0.0164\n",
            "Epoch [93/1000], Index: [3], Loss: 0.0163\n",
            "Epoch [93/1000], Index: [4], Loss: 0.0164\n",
            "Epoch [93/1000], Index: [5], Loss: 0.0162\n",
            "Epoch [94/1000], Index: [1], Loss: 0.0163\n",
            "Epoch [94/1000], Index: [2], Loss: 0.0163\n",
            "Epoch [94/1000], Index: [3], Loss: 0.0162\n",
            "Epoch [94/1000], Index: [4], Loss: 0.0162\n",
            "Epoch [94/1000], Index: [5], Loss: 0.0160\n",
            "Epoch [95/1000], Index: [1], Loss: 0.0161\n",
            "Epoch [95/1000], Index: [2], Loss: 0.0160\n",
            "Epoch [95/1000], Index: [3], Loss: 0.0161\n",
            "Epoch [95/1000], Index: [4], Loss: 0.0159\n",
            "Epoch [95/1000], Index: [5], Loss: 0.0161\n",
            "Epoch [96/1000], Index: [1], Loss: 0.0159\n",
            "Epoch [96/1000], Index: [2], Loss: 0.0159\n",
            "Epoch [96/1000], Index: [3], Loss: 0.0159\n",
            "Epoch [96/1000], Index: [4], Loss: 0.0157\n",
            "Epoch [96/1000], Index: [5], Loss: 0.0157\n",
            "Epoch [97/1000], Index: [1], Loss: 0.0157\n",
            "Epoch [97/1000], Index: [2], Loss: 0.0157\n",
            "Epoch [97/1000], Index: [3], Loss: 0.0157\n",
            "Epoch [97/1000], Index: [4], Loss: 0.0157\n",
            "Epoch [97/1000], Index: [5], Loss: 0.0157\n",
            "Epoch [98/1000], Index: [1], Loss: 0.0157\n",
            "Epoch [98/1000], Index: [2], Loss: 0.0155\n",
            "Epoch [98/1000], Index: [3], Loss: 0.0155\n",
            "Epoch [98/1000], Index: [4], Loss: 0.0155\n",
            "Epoch [98/1000], Index: [5], Loss: 0.0153\n",
            "Epoch [99/1000], Index: [1], Loss: 0.0155\n",
            "Epoch [99/1000], Index: [2], Loss: 0.0155\n",
            "Epoch [99/1000], Index: [3], Loss: 0.0154\n",
            "Epoch [99/1000], Index: [4], Loss: 0.0153\n",
            "Epoch [99/1000], Index: [5], Loss: 0.0153\n",
            "Epoch [100/1000], Index: [1], Loss: 0.0152\n",
            "Epoch [100/1000], Index: [2], Loss: 0.0152\n",
            "Epoch [100/1000], Index: [3], Loss: 0.0152\n",
            "Epoch [100/1000], Index: [4], Loss: 0.0150\n",
            "Epoch [100/1000], Index: [5], Loss: 0.0152\n",
            "Epoch [101/1000], Index: [1], Loss: 0.0151\n",
            "Epoch [101/1000], Index: [2], Loss: 0.0150\n",
            "Epoch [101/1000], Index: [3], Loss: 0.0151\n",
            "Epoch [101/1000], Index: [4], Loss: 0.0150\n",
            "Epoch [101/1000], Index: [5], Loss: 0.0150\n",
            "Epoch [102/1000], Index: [1], Loss: 0.0149\n",
            "Epoch [102/1000], Index: [2], Loss: 0.0148\n",
            "Epoch [102/1000], Index: [3], Loss: 0.0149\n",
            "Epoch [102/1000], Index: [4], Loss: 0.0149\n",
            "Epoch [102/1000], Index: [5], Loss: 0.0148\n",
            "Epoch [103/1000], Index: [1], Loss: 0.0148\n",
            "Epoch [103/1000], Index: [2], Loss: 0.0147\n",
            "Epoch [103/1000], Index: [3], Loss: 0.0147\n",
            "Epoch [103/1000], Index: [4], Loss: 0.0148\n",
            "Epoch [103/1000], Index: [5], Loss: 0.0147\n",
            "Epoch [104/1000], Index: [1], Loss: 0.0146\n",
            "Epoch [104/1000], Index: [2], Loss: 0.0146\n",
            "Epoch [104/1000], Index: [3], Loss: 0.0146\n",
            "Epoch [104/1000], Index: [4], Loss: 0.0145\n",
            "Epoch [104/1000], Index: [5], Loss: 0.0144\n",
            "Epoch [105/1000], Index: [1], Loss: 0.0144\n",
            "Epoch [105/1000], Index: [2], Loss: 0.0143\n",
            "Epoch [105/1000], Index: [3], Loss: 0.0144\n",
            "Epoch [105/1000], Index: [4], Loss: 0.0144\n",
            "Epoch [105/1000], Index: [5], Loss: 0.0142\n",
            "Epoch [106/1000], Index: [1], Loss: 0.0142\n",
            "Epoch [106/1000], Index: [2], Loss: 0.0144\n",
            "Epoch [106/1000], Index: [3], Loss: 0.0142\n",
            "Epoch [106/1000], Index: [4], Loss: 0.0143\n",
            "Epoch [106/1000], Index: [5], Loss: 0.0142\n",
            "Epoch [107/1000], Index: [1], Loss: 0.0142\n",
            "Epoch [107/1000], Index: [2], Loss: 0.0141\n",
            "Epoch [107/1000], Index: [3], Loss: 0.0141\n",
            "Epoch [107/1000], Index: [4], Loss: 0.0140\n",
            "Epoch [107/1000], Index: [5], Loss: 0.0140\n",
            "Epoch [108/1000], Index: [1], Loss: 0.0140\n",
            "Epoch [108/1000], Index: [2], Loss: 0.0139\n",
            "Epoch [108/1000], Index: [3], Loss: 0.0140\n",
            "Epoch [108/1000], Index: [4], Loss: 0.0139\n",
            "Epoch [108/1000], Index: [5], Loss: 0.0140\n",
            "Epoch [109/1000], Index: [1], Loss: 0.0139\n",
            "Epoch [109/1000], Index: [2], Loss: 0.0138\n",
            "Epoch [109/1000], Index: [3], Loss: 0.0138\n",
            "Epoch [109/1000], Index: [4], Loss: 0.0138\n",
            "Epoch [109/1000], Index: [5], Loss: 0.0138\n",
            "Epoch [110/1000], Index: [1], Loss: 0.0137\n",
            "Epoch [110/1000], Index: [2], Loss: 0.0136\n",
            "Epoch [110/1000], Index: [3], Loss: 0.0137\n",
            "Epoch [110/1000], Index: [4], Loss: 0.0136\n",
            "Epoch [110/1000], Index: [5], Loss: 0.0136\n",
            "Epoch [111/1000], Index: [1], Loss: 0.0136\n",
            "Epoch [111/1000], Index: [2], Loss: 0.0136\n",
            "Epoch [111/1000], Index: [3], Loss: 0.0136\n",
            "Epoch [111/1000], Index: [4], Loss: 0.0135\n",
            "Epoch [111/1000], Index: [5], Loss: 0.0134\n",
            "Epoch [112/1000], Index: [1], Loss: 0.0133\n",
            "Epoch [112/1000], Index: [2], Loss: 0.0133\n",
            "Epoch [112/1000], Index: [3], Loss: 0.0133\n",
            "Epoch [112/1000], Index: [4], Loss: 0.0133\n",
            "Epoch [112/1000], Index: [5], Loss: 0.0134\n",
            "Epoch [113/1000], Index: [1], Loss: 0.0131\n",
            "Epoch [113/1000], Index: [2], Loss: 0.0131\n",
            "Epoch [113/1000], Index: [3], Loss: 0.0131\n",
            "Epoch [113/1000], Index: [4], Loss: 0.0131\n",
            "Epoch [113/1000], Index: [5], Loss: 0.0131\n",
            "Epoch [114/1000], Index: [1], Loss: 0.0132\n",
            "Epoch [114/1000], Index: [2], Loss: 0.0131\n",
            "Epoch [114/1000], Index: [3], Loss: 0.0131\n",
            "Epoch [114/1000], Index: [4], Loss: 0.0131\n",
            "Epoch [114/1000], Index: [5], Loss: 0.0130\n",
            "Epoch [115/1000], Index: [1], Loss: 0.0129\n",
            "Epoch [115/1000], Index: [2], Loss: 0.0130\n",
            "Epoch [115/1000], Index: [3], Loss: 0.0128\n",
            "Epoch [115/1000], Index: [4], Loss: 0.0127\n",
            "Epoch [115/1000], Index: [5], Loss: 0.0128\n",
            "Epoch [116/1000], Index: [1], Loss: 0.0128\n",
            "Epoch [116/1000], Index: [2], Loss: 0.0128\n",
            "Epoch [116/1000], Index: [3], Loss: 0.0128\n",
            "Epoch [116/1000], Index: [4], Loss: 0.0127\n",
            "Epoch [116/1000], Index: [5], Loss: 0.0128\n",
            "Epoch [117/1000], Index: [1], Loss: 0.0127\n",
            "Epoch [117/1000], Index: [2], Loss: 0.0127\n",
            "Epoch [117/1000], Index: [3], Loss: 0.0127\n",
            "Epoch [117/1000], Index: [4], Loss: 0.0127\n",
            "Epoch [117/1000], Index: [5], Loss: 0.0125\n",
            "Epoch [118/1000], Index: [1], Loss: 0.0126\n",
            "Epoch [118/1000], Index: [2], Loss: 0.0124\n",
            "Epoch [118/1000], Index: [3], Loss: 0.0124\n",
            "Epoch [118/1000], Index: [4], Loss: 0.0125\n",
            "Epoch [118/1000], Index: [5], Loss: 0.0124\n",
            "Epoch [119/1000], Index: [1], Loss: 0.0125\n",
            "Epoch [119/1000], Index: [2], Loss: 0.0124\n",
            "Epoch [119/1000], Index: [3], Loss: 0.0123\n",
            "Epoch [119/1000], Index: [4], Loss: 0.0122\n",
            "Epoch [119/1000], Index: [5], Loss: 0.0122\n",
            "Epoch [120/1000], Index: [1], Loss: 0.0123\n",
            "Epoch [120/1000], Index: [2], Loss: 0.0122\n",
            "Epoch [120/1000], Index: [3], Loss: 0.0122\n",
            "Epoch [120/1000], Index: [4], Loss: 0.0122\n",
            "Epoch [120/1000], Index: [5], Loss: 0.0123\n",
            "Epoch [121/1000], Index: [1], Loss: 0.0120\n",
            "Epoch [121/1000], Index: [2], Loss: 0.0122\n",
            "Epoch [121/1000], Index: [3], Loss: 0.0121\n",
            "Epoch [121/1000], Index: [4], Loss: 0.0120\n",
            "Epoch [121/1000], Index: [5], Loss: 0.0120\n",
            "Epoch [122/1000], Index: [1], Loss: 0.0119\n",
            "Epoch [122/1000], Index: [2], Loss: 0.0120\n",
            "Epoch [122/1000], Index: [3], Loss: 0.0118\n",
            "Epoch [122/1000], Index: [4], Loss: 0.0119\n",
            "Epoch [122/1000], Index: [5], Loss: 0.0118\n",
            "Epoch [123/1000], Index: [1], Loss: 0.0118\n",
            "Epoch [123/1000], Index: [2], Loss: 0.0119\n",
            "Epoch [123/1000], Index: [3], Loss: 0.0117\n",
            "Epoch [123/1000], Index: [4], Loss: 0.0117\n",
            "Epoch [123/1000], Index: [5], Loss: 0.0118\n",
            "Epoch [124/1000], Index: [1], Loss: 0.0117\n",
            "Epoch [124/1000], Index: [2], Loss: 0.0117\n",
            "Epoch [124/1000], Index: [3], Loss: 0.0117\n",
            "Epoch [124/1000], Index: [4], Loss: 0.0116\n",
            "Epoch [124/1000], Index: [5], Loss: 0.0116\n",
            "Epoch [125/1000], Index: [1], Loss: 0.0115\n",
            "Epoch [125/1000], Index: [2], Loss: 0.0116\n",
            "Epoch [125/1000], Index: [3], Loss: 0.0115\n",
            "Epoch [125/1000], Index: [4], Loss: 0.0115\n",
            "Epoch [125/1000], Index: [5], Loss: 0.0114\n",
            "Epoch [126/1000], Index: [1], Loss: 0.0113\n",
            "Epoch [126/1000], Index: [2], Loss: 0.0114\n",
            "Epoch [126/1000], Index: [3], Loss: 0.0113\n",
            "Epoch [126/1000], Index: [4], Loss: 0.0114\n",
            "Epoch [126/1000], Index: [5], Loss: 0.0113\n",
            "Epoch [127/1000], Index: [1], Loss: 0.0113\n",
            "Epoch [127/1000], Index: [2], Loss: 0.0114\n",
            "Epoch [127/1000], Index: [3], Loss: 0.0112\n",
            "Epoch [127/1000], Index: [4], Loss: 0.0112\n",
            "Epoch [127/1000], Index: [5], Loss: 0.0113\n",
            "Epoch [128/1000], Index: [1], Loss: 0.0112\n",
            "Epoch [128/1000], Index: [2], Loss: 0.0111\n",
            "Epoch [128/1000], Index: [3], Loss: 0.0111\n",
            "Epoch [128/1000], Index: [4], Loss: 0.0112\n",
            "Epoch [128/1000], Index: [5], Loss: 0.0111\n",
            "Epoch [129/1000], Index: [1], Loss: 0.0110\n",
            "Epoch [129/1000], Index: [2], Loss: 0.0110\n",
            "Epoch [129/1000], Index: [3], Loss: 0.0110\n",
            "Epoch [129/1000], Index: [4], Loss: 0.0109\n",
            "Epoch [129/1000], Index: [5], Loss: 0.0109\n",
            "Epoch [130/1000], Index: [1], Loss: 0.0110\n",
            "Epoch [130/1000], Index: [2], Loss: 0.0108\n",
            "Epoch [130/1000], Index: [3], Loss: 0.0110\n",
            "Epoch [130/1000], Index: [4], Loss: 0.0109\n",
            "Epoch [130/1000], Index: [5], Loss: 0.0108\n",
            "Epoch [131/1000], Index: [1], Loss: 0.0108\n",
            "Epoch [131/1000], Index: [2], Loss: 0.0108\n",
            "Epoch [131/1000], Index: [3], Loss: 0.0107\n",
            "Epoch [131/1000], Index: [4], Loss: 0.0106\n",
            "Epoch [131/1000], Index: [5], Loss: 0.0107\n",
            "Epoch [132/1000], Index: [1], Loss: 0.0106\n",
            "Epoch [132/1000], Index: [2], Loss: 0.0108\n",
            "Epoch [132/1000], Index: [3], Loss: 0.0106\n",
            "Epoch [132/1000], Index: [4], Loss: 0.0106\n",
            "Epoch [132/1000], Index: [5], Loss: 0.0106\n",
            "Epoch [133/1000], Index: [1], Loss: 0.0105\n",
            "Epoch [133/1000], Index: [2], Loss: 0.0104\n",
            "Epoch [133/1000], Index: [3], Loss: 0.0105\n",
            "Epoch [133/1000], Index: [4], Loss: 0.0104\n",
            "Epoch [133/1000], Index: [5], Loss: 0.0105\n",
            "Epoch [134/1000], Index: [1], Loss: 0.0105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-412-85e5fc96aed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_LAYERS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDIRECTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-411-013bade8621a>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, epochs, state_dim)\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m           \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m           \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-408-38b39381004b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, states)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}